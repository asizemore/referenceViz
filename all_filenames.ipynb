{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all_references files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inport packages\n",
    "from pathlib import Path\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import logging\n",
    "import locale\n",
    "import dataclasses\n",
    "import copy\n",
    "from itertools import groupby\n",
    "from itertools import compress\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import jinja2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from DABI_databases import *\n",
    "\n",
    "\n",
    "## Functions\n",
    "# Remove known non-keywords from keyword list\n",
    "def filter_words(variable):\n",
    "    remove_words = ['a','and','of','the','in', \"The\", \"an\", \"An\", \"by\", \"le\", \"les\", \"on\", \"for\", \"this\",\n",
    "                    \"their\", \"most\", \"\\n  \\n\", \"\\n   \\n\", \"\\n     \\n\", \"these\", \"that\", \"many\", \"some\", \"these\", \"also\",\n",
    "                    \"such\", \"which\", \"palestine._id%c3%a9ologies_religieuses_entre_ugarit_et_le_monde_ph%c3%a9nicien_ugarit\"]\n",
    "    if (variable in remove_words):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>None</td>\n",
       "      <td>2011</td>\n",
       "      <td>[access, information, express, research, play,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abusch2020Corpus</td>\n",
       "      <td>Abusch, Tzvi; Daniel Schwemer, Mikko Lukko, an...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[proceed, dealing, January, interested, Brill,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Achtemeier1996Harper</td>\n",
       "      <td>Achtemeier, Paul J. *et al.* (eds)</td>\n",
       "      <td>1996</td>\n",
       "      <td>[Dictionary, contextualize, dictionary, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albani2000Horoscopes</td>\n",
       "      <td>Albani, Matthias</td>\n",
       "      <td>2000</td>\n",
       "      <td>[University, scroll, investigate, Schiffman, H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albright1940ANEreligion</td>\n",
       "      <td>Albright, W.F.</td>\n",
       "      <td>1940</td>\n",
       "      <td>[inherent, contrast, cross, Hittite, methodolo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename                                             author  \\\n",
       "0                     AMGG                                               None   \n",
       "1         Abusch2020Corpus  Abusch, Tzvi; Daniel Schwemer, Mikko Lukko, an...   \n",
       "2     Achtemeier1996Harper                 Achtemeier, Paul J. *et al.* (eds)   \n",
       "3     Albani2000Horoscopes                                   Albani, Matthias   \n",
       "4  Albright1940ANEreligion                                     Albright, W.F.   \n",
       "\n",
       "   year                                           keywords  \n",
       "0  2011  [access, information, express, research, play,...  \n",
       "1  2020  [proceed, dealing, January, interested, Brill,...  \n",
       "2  1996  [Dictionary, contextualize, dictionary, colour...  \n",
       "3  2000  [University, scroll, investigate, Schiffman, H...  \n",
       "4  1940  [inherent, contrast, cross, Hittite, methodolo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From Jonah's file\n",
    "\n",
    "MYPATH = \"./raw_data/\"\n",
    "\n",
    "#reference copy of previous working implementation of spaCy\n",
    "\n",
    "#this is a new NLP attempt, implementing spaCy to output one large csv with topics inside it\n",
    "#this version outputs parsed_dabi_filesNLP2.csv a csv with filename, author, year, and topic list\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from gensim.summarization import keywords\n",
    "import spacy\n",
    "\n",
    "#make a function to check for certain strings\n",
    "def check_if_string_in_file(file_name, string_to_search):\n",
    "    # Check if any line in the file contains given string\n",
    "    # Open the file in read only mode\n",
    "    with open(file_name, 'r') as read_obj:\n",
    "        # Read all lines in the file one by one\n",
    "        for line in read_obj:\n",
    "            # For each line, check if line contains the string\n",
    "            if string_to_search in line:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def search_string_in_file(file_name, string_to_search):\n",
    "    #Search for the given string in file and return the lines containing that string\n",
    "    #only works for unique strings in file\n",
    "    line_number = 0\n",
    "   \n",
    "    # Open the file in read only mode\n",
    "    with open(file_name, 'r') as read_obj:\n",
    "        # Read all lines in the file one by one\n",
    "        for line in read_obj:\n",
    "            # For each line, check if line contains the string\n",
    "            line_number += 1\n",
    "            if string_to_search in line:\n",
    "                # If yes, then add the line to the list\n",
    "               return line.rstrip()\n",
    "    \n",
    "\n",
    "#clear main lists\n",
    "filename = []\n",
    "author = []\n",
    "year = []\n",
    "topics = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#main program loop\n",
    "#loop through all the .d files in the directory\n",
    "for currentfile in os.listdir(MYPATH):\n",
    "    if fnmatch.fnmatch(currentfile, '*.d'):\n",
    "        data_folder = Path(MYPATH)\n",
    "        file_to_open = data_folder / currentfile\n",
    "\n",
    "        f = open(file_to_open)\n",
    "        # Check if string '@@@R' is found in file, which means we are referring to material for the Mes/Rel site\n",
    "        if check_if_string_in_file(file_to_open, '@@@R'):\n",
    "\n",
    "            #take the filename for each @@@R file and populate the lists (cutting off .d extension)\n",
    "            filename.append(currentfile[:-2])\n",
    "            \n",
    "            #choose AU (author) line and append to list\n",
    "            au = search_string_in_file(file_to_open, 'AU ')\n",
    "            if au is None: #make sure we don't get null values in list \n",
    "                au = 'AU None'\n",
    "            author.append(au[3:])\n",
    "            \n",
    "            #choose Y (year) line and append to list    \n",
    "            y = search_string_in_file(file_to_open, 'Y ')\n",
    "            if y is None: #make sure we don't get null values in list \n",
    "                y = 'Y None'\n",
    "            year.append(y[2:6]) #at most allow four digits for year\n",
    "               \n",
    "            #NLP routine to parse all nouns in the current file\n",
    "            doc = nlp(f.read())\n",
    "                \n",
    "            t = []\n",
    "            topic_list = []\n",
    "    \n",
    "            for t in doc:\n",
    "                \n",
    "                if len(t)<4: #get rid of short words\n",
    "                    continue\n",
    "                if t.pos_ == 'NUM' or t.pos_ == 'AUX' or t.pos_ == 'ADP' or t.pos_ == 'SYM' or t.pos_ == 'AUX': #get rid of numbers, auxiliary verbs, prepositions, symbols\n",
    "                    continue\n",
    "                if t.text.find(\"http\") != -1: #get rid of hyperlinks\n",
    "                    continue\n",
    "                if t.text.find(\"@\") != -1: #get rid of site references\n",
    "                    continue\n",
    "                if t.text.find(\".pdf\") != -1: #get rid of file references\n",
    "                    continue\n",
    "                if t.text.find(\".html\") != -1: #get rid of file references\n",
    "                    continue\n",
    "                if t.text.find(\"R/\") != -1: #get rid of reference codes\n",
    "                    continue     \n",
    "                if t.text.find(\"\\n\\n\") != -1: #get rid of carriage returns\n",
    "                    continue\n",
    "                if t.pos_ == \"PROPN\":  #put proper names in the list with capital letter\n",
    "                    topic_list.append(t.lemma_)\n",
    "                    continue\n",
    "                \n",
    "                topicToAdd = t.lemma_\n",
    "                topicToAdd = topicToAdd.lower()  #make everything except proper names lowercase\n",
    "                topic_list.append(topicToAdd)    #append the lemma of each word to the list\n",
    "                \n",
    "                # Remove words\n",
    "\n",
    "                \n",
    "            topics.append(topic_list)\n",
    " \n",
    "a = {'filename': filename, 'author': author, 'year': year, 'keywords': [list(set(list(filter(filter_words,topic)))) for topic in topics]}\n",
    "\n",
    "df = pd.DataFrame(a, columns=['filename','author','year', 'keywords'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter based on filename and write to csv\n",
    "df.to_csv(\"./data/all_filenames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 72451 edges.\n"
     ]
    }
   ],
   "source": [
    "## Determine edges -- very slow\n",
    "\n",
    "# Loop through filenames and add an edge if they share a keyword\n",
    "nNodes = df.shape[0]\n",
    "sourceList = []\n",
    "targetList = []\n",
    "keywordList = []\n",
    "\n",
    "for i in np.arange(nNodes):\n",
    "\n",
    "    for j in np.arange(i+1,nNodes):\n",
    "        \n",
    "        keywords_in_i = df.keywords[i]\n",
    "        keywords_in_j = df.keywords[j]\n",
    "\n",
    "        # If any of keywords in i and j\n",
    "        if any([keywords_in_i[ind] in keywords_in_j for ind in np.arange(len(keywords_in_i))]):\n",
    "            sourceList.append(df.filename[i])\n",
    "            targetList.append(df.filename[j])\n",
    "            keywordList.append(list(set(list(compress(keywords_in_i, [keywords_in_i[ind] in keywords_in_j for ind in np.arange(len(keywords_in_i))])))))\n",
    "                \n",
    "\n",
    "            \n",
    "edge_df = pd.DataFrame({'source': sourceList, 'target': targetList, 'keywords': keywordList})\n",
    "print(f'Identified {edge_df.shape[0]} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>Abusch2020Corpus</td>\n",
       "      <td>[corpus, August, mesopotamian, literature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>Achtemeier1996Harper</td>\n",
       "      <td>[further, offer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>Albani2000Horoscopes</td>\n",
       "      <td>[religion, August]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>Albright1940ANEreligion</td>\n",
       "      <td>[research, literature, polytheistic, ancient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMGG</td>\n",
       "      <td>Allegro1968DJD5</td>\n",
       "      <td>[offer, literature, part]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                   target  \\\n",
       "0   AMGG         Abusch2020Corpus   \n",
       "1   AMGG     Achtemeier1996Harper   \n",
       "2   AMGG     Albani2000Horoscopes   \n",
       "3   AMGG  Albright1940ANEreligion   \n",
       "4   AMGG          Allegro1968DJD5   \n",
       "\n",
       "                                            keywords  \n",
       "0         [corpus, August, mesopotamian, literature]  \n",
       "1                                   [further, offer]  \n",
       "2                                 [religion, August]  \n",
       "3  [research, literature, polytheistic, ancient, ...  \n",
       "4                          [offer, literature, part]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to csv\n",
    "edge_df.to_csv(\"./data/all_filenames_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
