{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all_keywords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DABI_databases'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43fce0a0030c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDABI_databases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DABI_databases'"
     ]
    }
   ],
   "source": [
    "## Inport packages\n",
    "from pathlib import Path\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import logging\n",
    "import locale\n",
    "import dataclasses\n",
    "import copy\n",
    "from itertools import groupby\n",
    "from itertools import compress\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import jinja2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from DABI_databases import *\n",
    "\n",
    "\n",
    "## Functions\n",
    "# Remove known non-keywords from keyword list\n",
    "def filter_words(variable):\n",
    "    remove_words = ['a','and','of','the','in', \"The\", \"an\", \"An\", \"by\", \"le\", \"les\", \"on\", \"for\", \"this\",\n",
    "                    \"their\", \"most\", \"\\n  \\n\", \"\\n   \\n\", \"\\n     \\n\", \"these\", \"that\", \"many\", \"some\", \"these\", \"also\",\n",
    "                    \"such\", \"which\", \"palestine._id%c3%a9ologies_religieuses_entre_ugarit_et_le_monde_ph%c3%a9nicien_ugarit\"]\n",
    "    if (variable in remove_words):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Jonah's file\n",
    "\n",
    "MYPATH = \"./raw_data/\"\n",
    "\n",
    "#reference copy of previous working implementation of spaCy\n",
    "\n",
    "#this is a new NLP attempt, implementing spaCy to output one large csv with topics inside it\n",
    "#this version outputs parsed_dabi_filesNLP2.csv a csv with filename, author, year, and topic list\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from gensim.summarization import keywords\n",
    "import spacy\n",
    "\n",
    "#make a function to check for certain strings\n",
    "def check_if_string_in_file(file_name, string_to_search):\n",
    "    # Check if any line in the file contains given string\n",
    "    # Open the file in read only mode\n",
    "    with open(file_name, 'r') as read_obj:\n",
    "        # Read all lines in the file one by one\n",
    "        for line in read_obj:\n",
    "            # For each line, check if line contains the string\n",
    "            if string_to_search in line:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def search_string_in_file(file_name, string_to_search):\n",
    "    #Search for the given string in file and return the lines containing that string\n",
    "    #only works for unique strings in file\n",
    "    line_number = 0\n",
    "   \n",
    "    # Open the file in read only mode\n",
    "    with open(file_name, 'r') as read_obj:\n",
    "        # Read all lines in the file one by one\n",
    "        for line in read_obj:\n",
    "            # For each line, check if line contains the string\n",
    "            line_number += 1\n",
    "            if string_to_search in line:\n",
    "                # If yes, then add the line to the list\n",
    "               return line.rstrip()\n",
    "    \n",
    "\n",
    "#clear main lists\n",
    "filename = []\n",
    "author = []\n",
    "year = []\n",
    "topics = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#main program loop\n",
    "#loop through all the .d files in the directory\n",
    "for currentfile in os.listdir(MYPATH):\n",
    "    if fnmatch.fnmatch(currentfile, '*.d'):\n",
    "        data_folder = Path(MYPATH)\n",
    "        file_to_open = data_folder / currentfile\n",
    "\n",
    "        f = open(file_to_open)\n",
    "        # Check if string '@@@R' is found in file, which means we are referring to material for the Mes/Rel site\n",
    "        if check_if_string_in_file(file_to_open, '@@@R'):\n",
    "\n",
    "            #take the filename for each @@@R file and populate the lists (cutting off .d extension)\n",
    "            filename.append(currentfile[:-2])\n",
    "            \n",
    "            #choose AU (author) line and append to list\n",
    "            au = search_string_in_file(file_to_open, 'AU ')\n",
    "            if au is None: #make sure we don't get null values in list \n",
    "                au = 'AU None'\n",
    "            author.append(au[3:])\n",
    "            \n",
    "            #choose Y (year) line and append to list    \n",
    "            y = search_string_in_file(file_to_open, 'Y ')\n",
    "            if y is None: #make sure we don't get null values in list \n",
    "                y = 'Y None'\n",
    "            year.append(y[2:6]) #at most allow four digits for year\n",
    "               \n",
    "            #NLP routine to parse all nouns in the current file\n",
    "            doc = nlp(f.read())\n",
    "                \n",
    "            t = []\n",
    "            topic_list = []\n",
    "    \n",
    "            for t in doc:\n",
    "                \n",
    "                if len(t)<4: #get rid of short words\n",
    "                    continue\n",
    "                if t.pos_ == 'NUM' or t.pos_ == 'AUX' or t.pos_ == 'ADP' or t.pos_ == 'SYM' or t.pos_ == 'AUX': #get rid of numbers, auxiliary verbs, prepositions, symbols\n",
    "                    continue\n",
    "                if t.text.find(\"http\") != -1: #get rid of hyperlinks\n",
    "                    continue\n",
    "                if t.text.find(\"@\") != -1: #get rid of site references\n",
    "                    continue\n",
    "                if t.text.find(\".pdf\") != -1: #get rid of file references\n",
    "                    continue\n",
    "                if t.text.find(\".html\") != -1: #get rid of file references\n",
    "                    continue\n",
    "                if t.text.find(\"R/\") != -1: #get rid of reference codes\n",
    "                    continue     \n",
    "                if t.text.find(\"\\n\\n\") != -1: #get rid of carriage returns\n",
    "                    continue\n",
    "                if t.pos_ == \"PROPN\":  #put proper names in the list with capital letter\n",
    "                    topic_list.append(t.lemma_)\n",
    "                    continue\n",
    "                \n",
    "                topicToAdd = t.lemma_\n",
    "                topicToAdd = topicToAdd.lower()  #make everything except proper names lowercase\n",
    "                topic_list.append(topicToAdd)    #append the lemma of each word to the list\n",
    "                \n",
    "                # Remove words\n",
    "\n",
    "                \n",
    "            topics.append(topic_list)\n",
    " \n",
    "a = {'filename': filename, 'author': author, 'year': year, 'keywords': [list(filter(filter_words,topic)) for topic in topics]}\n",
    "\n",
    "df = pd.DataFrame(a, columns=['filename','author','year', 'keywords'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of keywords -- will become the nodes\n",
    "\n",
    "all_keywords = np.unique([item for sublist in df.keywords for item in sublist])\n",
    "all_keywords = all_keywords[58:]  # I noticed later that the first bunch are not helpful\n",
    "print(f'Identified {all_keywords.shape[0]} nodes (keywords)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape the data so that we have one row per keyword\n",
    "\n",
    "file_list = []\n",
    "author_list = []\n",
    "keyword_list = []\n",
    "edge_strength = []\n",
    "\n",
    "for r in np.arange(len(all_keywords)):\n",
    "\n",
    "    filenames_for_keyword = []\n",
    "    matching_rows = [ind for (ind,value) in enumerate(df.keywords) if all_keywords[r] in value]\n",
    "\n",
    "    file_list.append(list(df.filename[matching_rows]))\n",
    "    author_list.append(list(df.author[matching_rows]))\n",
    "\n",
    "    \n",
    "df_keywords = pd.DataFrame({'keyword': all_keywords, 'filenames': file_list, 'authors': author_list})\n",
    "df_keywords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to csv\n",
    "df_keywords.to_csv(\"./data/all_keywords2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine edges -- very slow\n",
    "\n",
    "# Loop through keywords and add an edge if they are mentioned within the same reference\n",
    "nNodes = df_keywords.shape[0]\n",
    "sourceList = []\n",
    "targetList = []\n",
    "filenameList = []\n",
    "\n",
    "for i in np.arange(nNodes):\n",
    "\n",
    "    for j in np.arange(i+1,nNodes):\n",
    "        \n",
    "        files_in_i = df_keywords.filenames[i]\n",
    "        files_in_j = df_keywords.filenames[j]\n",
    "\n",
    "        # If any of filenames in i and j\n",
    "        if any([files_in_i[ind] in files_in_j for ind in np.arange(len(files_in_i))]):\n",
    "            sourceList.append(df_keywords.keyword[i])\n",
    "            targetList.append(df_keywords.keyword[j])\n",
    "            filenameList.append(list(compress(files_in_i, [files_in_i[ind] in files_in_j for ind in np.arange(len(files_in_i))])))\n",
    "                \n",
    "\n",
    "            \n",
    "edge_df = pd.DataFrame({'source': sourceList, 'target': targetList, 'files': filenameList})\n",
    "print(f'Identified {edge_df.shape[0]} edges.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
